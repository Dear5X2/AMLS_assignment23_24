import numpy as np
import matplotlib.pyplot as plt
import cv2
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.utils import to_categorical

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

#Loading the NPZ file
data = np.load('../AMLS_23-24_SN20019229/Datasets/pathmnist.npz')
#print(data.keys()) #Get keys of the dataset

#split data
X_train,X_val,X_test = data['train_images'],data['val_images'],data['test_images']
y_train,y_val,y_test = data['train_labels'].flatten(),data['val_labels'].flatten(),data['test_labels'].flatten()

#Normalizing
X_train, X_val, X_test = X_train / 255.0, X_val / 255.0, X_test / 255.0

y_train_one_hot = to_categorical(y_train, num_classes=9)
y_val_one_hot = to_categorical(y_val, num_classes=9)
y_test_one_hot = to_categorical(y_test, num_classes=9)

# Build the CNN model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(9, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train_one_hot, epochs=10, 
                    validation_data=(X_val, y_val_one_hot))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test,  y_test_one_hot, verbose=2)
print('\nTest accuracy:', test_acc)

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Predict the results
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_classes)

# Custom labels for the issues
issue_labels = ['Issue_0', 'Issue_1', 'Issue_2', 'Issue_3', 'Issue_4', 'Issue_5', 'Issue_6', 'Issue_7', 'Issue_8']

# Convert confusion matrix to logarithmic scale to handle wide value ranges
cm_log_scale = np.log1p(cm)  # log1p is used to handle zero values in the matrix

plt.figure(figsize=(10,8))
sns.heatmap(cm, fmt='d', cmap='Blues', xticklabels=issue_labels, yticklabels=issue_labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')

# Annotate each cell with the actual values from 'cm'
for i in range(cm_log_scale.shape[0]):
    for j in range(cm_log_scale.shape[1]):
        plt.text(j+0.5, i+0.5, cm[i, j], 
                 fontdict={'fontsize':8, 'weight':'bold', 'color':'black'},
                 ha='center', va='center')

plt.show()

# Classification Report
report = classification_report(y_test, y_pred_classes, digits=2)
    
# Classification Report
print(classification_report(y_test, y_pred_classes))
